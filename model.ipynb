{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "train_var = pd.read_csv('/Users/senchen/Desktop/ds/training_variants')\n",
    "test_var = pd.read_csv('/Users/senchen/Desktop/ds/test_variants')\n",
    "\n",
    "train_text = pd.read_csv('/Users/senchen/Desktop/ds/training_text',sep='\\|\\|',\n",
    "                    skiprows=1,engine='python',names=[\"ID\",\"text\"],encoding=\"utf-8\")\n",
    "test_text = pd.read_csv('/Users/senchen/Desktop/ds/test_text',sep='\\|\\|',\n",
    "                    skiprows=1,engine='python',names=[\"ID\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ID_train = train_var.ID\n",
    "ID_test = test_var.ID\n",
    "y = train_var.Class.values -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def clean_text(txt):\n",
    "    txt = re.sub(\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", txt)\n",
    "    txt =txt.lower().split()\n",
    "    txt = [s for s in txt if not s in stop_words]\n",
    "    txt = ' '.join(txt)\n",
    "    txt =txt.replace(',',\" \")\n",
    "    txt = txt.replace('.',\" \")\n",
    "    return(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tra_text = []#list\n",
    "tes_text = []#list\n",
    "for item in train_text['text']:\n",
    "    tra_text.append(clean_text(item))\n",
    "for item in test_text['text']:\n",
    "    tes_text.append(clean_text(item)) \n",
    "    \n",
    "All_text = pd.concat((train_text,test_text), axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gene_position(gene, txt,pos):\n",
    "    i = pos\n",
    "    while i != (len(txt)-1):\n",
    "        if (txt[i] == gene):\n",
    "            return i\n",
    "        else:\n",
    "            i += 1\n",
    "    return \"Not Found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nei(pos,n,txt):\n",
    "    result = []\n",
    "    if (pos-n) >= 0 & (pos+n) <= len(txt) +1:\n",
    "        left = pos-n\n",
    "        right = pos+n+1\n",
    "        result.append(txt[left:right])\n",
    "    elif (pos-n <0) & (pos + n) <= len(txt)+1:\n",
    "        left = 0\n",
    "        right = pos + n+1\n",
    "        result.append(txt[left:right])\n",
    "    elif (pos + n) > (len(txt)+1)  & (pos -n >=0):\n",
    "        left = pos-n\n",
    "        right = len(txt) +1\n",
    "        result.append(txt[left:right])\n",
    "    elif (pos +n) >(len(txt)+1) & (pos - n <= 0):\n",
    "        left =0\n",
    "        right = len(txt)\n",
    "        result.append(txt[left:right])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_new_text(gene,txt,n):\n",
    "    if gene_position(gene,txt,0) == \"Not Found\":\n",
    "        gene= \"mutation\"\n",
    "    pos = 0\n",
    "    result=\"\"\n",
    "    while pos <= len(txt):\n",
    "        pos = gene_position(gene,txt,pos)\n",
    "        if (pos != \"Not Found\"):\n",
    "            result = result + ' '.join(get_nei(pos,n,txt)[0])\n",
    "            pos += 1\n",
    "        elif (pos == \"Not Found\"):\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mutation_text(gene,txt,n):\n",
    "    if gene_position(gene,txt,0) == \"Not Found\":\n",
    "        return ''\n",
    "    pos = 0\n",
    "    result=\"\"\n",
    "    while pos <= len(txt):\n",
    "        pos = gene_position(gene,txt,pos)\n",
    "        if (pos != \"Not Found\"):\n",
    "            result = result + ' '.join(get_nei(pos,n,txt)[0])\n",
    "            pos += 1\n",
    "        elif (pos == \"Not Found\"):\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_tra_fea =[]\n",
    "new_tes_fea =[]\n",
    "for i in range(3321):\n",
    "    if build_new_text(train_var['Gene'][i].lower(),tra_text[i].split(),30) != '':\n",
    "        new_tra_fea.append(build_new_text(train_var['Gene'][i].lower(),tra_text[i].split(),30))\n",
    "    else:\n",
    "        new_tra_fea.append(tra_text[i])\n",
    "        \n",
    "for i in range(5668):\n",
    "    if build_new_text(test_var['Gene'][i].lower(),tes_text[i].split(),30) != '':\n",
    "        new_tes_fea.append(build_new_text(test_var['Gene'][i].lower(),tes_text[i].split(),30))\n",
    "    else:\n",
    "        new_tes_fea.append(tes_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tra_vari_text =[]\n",
    "tes_vari_text =[]\n",
    "for i in range(3321):\n",
    "    if build_new_text(train_var['Variation'][i].lower(),tra_text[i].split(),30) != '':\n",
    "        tra_vari_text.append(build_new_text(train_var['Variation'][i].lower(),tra_text[i].split(),30))\n",
    "    else:\n",
    "        tra_vari_text.append(tra_text[i])\n",
    "        #tra_vari_text.append(\"\")\n",
    "for i in range(5668):\n",
    "    if build_new_text(test_var['Variation'][i].lower(),tes_text[i].split(),30) != '':\n",
    "        tes_vari_text.append(build_new_text(test_var['Variation'][i].lower(),tes_text[i].split(),30))\n",
    "    else:\n",
    "        tes_vari_text.append(tes_text[i])\n",
    "        #tes_vari_text.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tra_muts_text =[]\n",
    "tes_muts_text =[]\n",
    "for i in range(3321):\n",
    "    if build_mutation_text(\"mutations\",tra_text[i].split(),30) != '':\n",
    "        tra_muts_text.append(build_mutation_text(\"mutations\",tra_text[i].split(),30))\n",
    "    else:\n",
    "        tra_muts_text.append(\"\")\n",
    "        \n",
    "for i in range(5668):\n",
    "    if build_mutation_text(\"mutations\",tes_text[i].split(),30) != '':\n",
    "        tes_muts_text.append(build_mutation_text(\"mutations\",tes_text[i].split(),30))\n",
    "    else:\n",
    "        tes_muts_text.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tra_cells_text =[]\n",
    "tes_cells_text =[]\n",
    "for i in range(3321):\n",
    "    if build_mutation_text(\"cells\",tra_text[i].split(),30) != '':\n",
    "        tra_cells_text.append(build_mutation_text(\"cells\",tra_text[i].split(),30))\n",
    "    else:\n",
    "        tra_cells_text.append(\"\")\n",
    "        \n",
    "for i in range(5668):\n",
    "    if build_mutation_text(\"cells\",tes_text[i].split(),30) != '':\n",
    "        tes_cells_text.append(build_mutation_text(\"cells\",tes_text[i].split(),30))\n",
    "    else:\n",
    "        tes_cells_text.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tra_cell_text =[]\n",
    "tes_cell_text =[]\n",
    "for i in range(3321):\n",
    "    if build_mutation_text(\"cell\",tra_text[i].split(),30) != '':\n",
    "        tra_cell_text.append(build_mutation_text(\"cell\",tra_text[i].split(),30))\n",
    "    else:\n",
    "        tra_cell_text.append(\"\")\n",
    "        \n",
    "for i in range(5668):\n",
    "    if build_mutation_text(\"cell\",tes_text[i].split(),30) != '':\n",
    "        tes_cell_text.append(build_mutation_text(\"cell\",tes_text[i].split(),30))\n",
    "    else:\n",
    "        tes_cell_text.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tra_can_text =[]\n",
    "tes_can_text =[]\n",
    "for i in range(3321):\n",
    "    if build_mutation_text(\"cancer\",tra_text[i].split(),30) != '':\n",
    "        tra_can_text.append(build_mutation_text(\"cancer\",tra_text[i].split(),30))\n",
    "    else:\n",
    "        tra_can_text.append(\"\")\n",
    "        \n",
    "for i in range(5668):\n",
    "    if build_mutation_text(\"cancer\",tes_text[i].split(),30) != '':\n",
    "        tes_can_text.append(build_mutation_text(\"cancer\",tes_text[i].split(),30))\n",
    "    else:\n",
    "        tes_can_text.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tra_x_text =[]\n",
    "tes_x_text =[]\n",
    "for i in range(3321):\n",
    "    if build_mutation_text(\"cancer\",tra_text[i].split(),30) != '':\n",
    "        tra_can_text.append(build_mutation_text(\"cancer\",tra_text[i].split(),30))\n",
    "    else:\n",
    "        tra_can_text.append(\"\")\n",
    "        \n",
    "for i in range(5668):\n",
    "    if build_mutation_text(\"cancer\",tes_text[i].split(),30) != '':\n",
    "        tes_can_text.append(build_mutation_text(\"cancer\",tes_text[i].split(),30))\n",
    "    else:\n",
    "        tes_can_text.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_word(var, words):\n",
    "    if var.lower() in words:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_count_gene = []\n",
    "train_count_vari =[]\n",
    "for i in range(len(train_var)):\n",
    "    train_count_gene.append(count_word(train_var['Gene'][i],tra_text[i]))\n",
    "\n",
    "for i in range(len(train_var)):\n",
    "    train_count_vari.append(count_word(train_var['Variation'][i],tra_text[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_count_gene = []\n",
    "test_count_vari =[]\n",
    "for i in range(len(test_var)):\n",
    "    test_count_gene.append(count_word(test_var['Gene'][i],tes_text[i]))\n",
    "\n",
    "for i in range(len(test_var)):\n",
    "    test_count_vari.append(count_word(test_var['Variation'][i],tes_text[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_clean_text = {'text' : tra_text}\n",
    "df_train_clean_text = pd.DataFrame.from_dict(train_clean_text)\n",
    "test_clean_text = {'text' : tes_text}\n",
    "df_test_clean_text = pd.DataFrame.from_dict(test_clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_new_text = pd.DataFrame(data=new_tra_fea,columns=['text'])\n",
    "df_test_new_text = pd.DataFrame(data=new_tes_fea,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_vari_text = pd.DataFrame(data=tra_vari_text,columns=['text'])\n",
    "df_test_vari_text = pd.DataFrame(data=tes_vari_text,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_muts_text = pd.DataFrame(data=tra_muts_text,columns=['text'])\n",
    "df_test_muts_text = pd.DataFrame(data=tes_muts_text,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_cells_text = pd.DataFrame(data=tra_cells_text,columns=['text'])\n",
    "df_test_cells_text = pd.DataFrame(data=tes_cells_text,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_cell_text = pd.DataFrame(data=tra_cell_text,columns=['text'])\n",
    "df_test_cell_text = pd.DataFrame(data=tes_cell_text,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_can_text = pd.DataFrame(data=tra_can_text,columns=['text'])\n",
    "df_test_can_text = pd.DataFrame(data=tes_can_text,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_x_text = pd.DataFrame(data=tra_x_text,columns=['text'])\n",
    "df_test_x_text = pd.DataFrame(data=tes_x_text,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT_var = pd.concat((train_var.drop(['ID','Class'],axis=1),test_var.drop(['ID'],axis=1)), axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT_text = pd.concat((df_train_clean_text,df_test_clean_text),axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT_new_text = pd.concat((df_train_new_text,df_test_new_text),axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT_vari_text = pd.concat((df_train_vari_text,df_test_vari_text),axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT_muts_text = pd.concat((df_train_muts_text,df_test_muts_text),axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT_cells_text = pd.concat((df_train_cells_text,df_test_cells_text),axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT_cell_text = pd.concat((df_train_cell_text,df_test_cell_text),axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT_can_text = pd.concat((df_train_can_text,df_test_can_text),axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT_x_text = pd.concat((df_train_x_text,df_test_x_text),axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf1 = TfidfVectorizer(min_df=5, ngram_range=(1,3), max_features=1000,\n",
    "                        strip_accents='unicode',\n",
    "                        lowercase =True, analyzer='word', token_pattern=r'\\w+',\n",
    "                        use_idf=True, smooth_idf=True, sublinear_tf=True, \n",
    "                        stop_words = 'english').fit(TT_new_text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf5= TfidfVectorizer(min_df=5, ngram_range=(1,3), max_features=1000,\n",
    "                        strip_accents='unicode',\n",
    "                        lowercase =True, analyzer='word', token_pattern=r'\\w+',\n",
    "                        use_idf=True, smooth_idf=True, sublinear_tf=True, \n",
    "                        stop_words = 'english').fit(TT_vari_text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf7= TfidfVectorizer(min_df=5, ngram_range=(1,3), max_features=1000,\n",
    "                        strip_accents='unicode',\n",
    "                        lowercase =True, analyzer='word', token_pattern=r'\\w+',\n",
    "                        use_idf=True, smooth_idf=True, sublinear_tf=True, \n",
    "                        stop_words = 'english').fit(TT_muts_text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf8= TfidfVectorizer(min_df=5, ngram_range=(1,3), max_features=1000,\n",
    "                        strip_accents='unicode',\n",
    "                        lowercase =True, analyzer='word', token_pattern=r'\\w+',\n",
    "                        use_idf=True, smooth_idf=True, sublinear_tf=True, \n",
    "                        stop_words = 'english').fit(TT_cells_text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf9= TfidfVectorizer(min_df=5, ngram_range=(1,3), max_features=1000,\n",
    "                        strip_accents='unicode',\n",
    "                        lowercase =True, analyzer='word', token_pattern=r'\\w+',\n",
    "                        use_idf=True, smooth_idf=True, sublinear_tf=True, \n",
    "                        stop_words = 'english').fit(TT_cell_text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf10= TfidfVectorizer(min_df=5, ngram_range=(1,3), max_features=1000,\n",
    "                        strip_accents='unicode',\n",
    "                        lowercase =True, analyzer='word', token_pattern=r'\\w+',\n",
    "                        use_idf=True, smooth_idf=True, sublinear_tf=True, \n",
    "                        stop_words = 'english').fit(TT_can_text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize, Imputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import scipy.sparse as ssp\n",
    "import math\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import preprocessing\n",
    "def get_features(vari,text,tx):\n",
    "    temp = vari.copy()\n",
    "    lbl = LabelEncoder()\n",
    "    temp['Gene_len'] = temp['Gene'].map(lambda x : len(x))\n",
    "    temp['Gene_lbl_cnc'] = lbl.fit_transform(temp['Gene'].values)\n",
    "    temp['Var_len'] = temp['Variation'].map(lambda x : len(x))\n",
    "    temp['Var_lbl_cnc'] = lbl.fit_transform(temp['Variation'].values)\n",
    "    temp['text_len'] =  tx['text'].map(lambda x : len(x))\n",
    "    temp['text_word_count'] =  tx['text'].map(lambda x : len(str(x).split(' ')))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT_data = get_features(TT_var,TT_new_text['text'],All_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT_data = TT_data.drop(['Gene'],axis=1)\n",
    "TT_data = TT_data.drop(['Variation'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene_len</th>\n",
       "      <th>Gene_lbl_cnc</th>\n",
       "      <th>Var_len</th>\n",
       "      <th>Var_lbl_cnc</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>447</td>\n",
       "      <td>20</td>\n",
       "      <td>7654</td>\n",
       "      <td>39672</td>\n",
       "      <td>6105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>216</td>\n",
       "      <td>5</td>\n",
       "      <td>8255</td>\n",
       "      <td>36691</td>\n",
       "      <td>5783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>216</td>\n",
       "      <td>5</td>\n",
       "      <td>5191</td>\n",
       "      <td>36691</td>\n",
       "      <td>5783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>216</td>\n",
       "      <td>5</td>\n",
       "      <td>4572</td>\n",
       "      <td>36238</td>\n",
       "      <td>5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>216</td>\n",
       "      <td>5</td>\n",
       "      <td>3958</td>\n",
       "      <td>41308</td>\n",
       "      <td>6248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gene_len  Gene_lbl_cnc  Var_len  Var_lbl_cnc  text_len  text_word_count\n",
       "0         6           447       20         7654     39672             6105\n",
       "1         3           216        5         8255     36691             5783\n",
       "2         3           216        5         5191     36691             5783\n",
       "3         3           216        5         4572     36238             5625\n",
       "4         3           216        5         3958     41308             6248"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TT_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_text_tfidf = tfidf1.transform(TT_new_text['text']) #reduced text gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vari_text_tfidf = tfidf5.transform(TT_vari_text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "muts_text_tfidf = tfidf7.transform(TT_muts_text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cells_text_tfidf = tfidf8.transform(TT_cells_text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell_text_tfidf = tfidf9.transform(TT_cells_text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "can_text_tfidf = tfidf10.transform(TT_can_text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components=52, random_state=2016)\n",
    "tfidf_tsvd1 =tsvd.fit_transform(new_text_tfidf)\n",
    "for i in range(tfidf_tsvd1.shape[1]):\n",
    "    TT_data['new_tfidf_tsvd' + str(i)] = tfidf_tsvd1[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components=52, random_state=2016)\n",
    "tfidf_tsvd5 =tsvd.fit_transform(vari_text_tfidf)\n",
    "for i in range(tfidf_tsvd5.shape[1]):\n",
    "    TT_data['vari_tfidf_tsvd' + str(i)] = tfidf_tsvd5[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components=52, random_state=2016)\n",
    "tfidf_tsvd7 =tsvd.fit_transform(muts_text_tfidf)\n",
    "for i in range(tfidf_tsvd7.shape[1]):\n",
    "    TT_data['muts_tfidf_tsvd' + str(i)] = tfidf_tsvd7[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components=52, random_state=2016)\n",
    "tfidf_tsvd8 =tsvd.fit_transform(cells_text_tfidf)\n",
    "for i in range(tfidf_tsvd8.shape[1]):\n",
    "    TT_data['cells_tfidf_tsvd' + str(i)] = tfidf_tsvd8[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BOW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(stop_words='english', max_features =1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_text_cv = cv.fit_transform(TT_new_text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components=52, random_state=2016)\n",
    "bow_tsvd = tsvd.fit_transform(new_text_cv)\n",
    "for i in range(bow_tsvd.shape[1]):\n",
    "    TT_data['new_bow_tsvd' + str(i)] = bow_tsvd[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_sim = [cosine_similarity(new_text_tfidf[i],vari_text_tfidf[i])[0][0] for i in range(new_text_tfidf.shape[0])]\n",
    "TT_data['cos_sim'] = cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene_len</th>\n",
       "      <th>Gene_lbl_cnc</th>\n",
       "      <th>Var_len</th>\n",
       "      <th>Var_lbl_cnc</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>new_tfidf_tsvd0</th>\n",
       "      <th>new_tfidf_tsvd1</th>\n",
       "      <th>new_tfidf_tsvd2</th>\n",
       "      <th>new_tfidf_tsvd3</th>\n",
       "      <th>...</th>\n",
       "      <th>new_bow_tsvd43</th>\n",
       "      <th>new_bow_tsvd44</th>\n",
       "      <th>new_bow_tsvd45</th>\n",
       "      <th>new_bow_tsvd46</th>\n",
       "      <th>new_bow_tsvd47</th>\n",
       "      <th>new_bow_tsvd48</th>\n",
       "      <th>new_bow_tsvd49</th>\n",
       "      <th>new_bow_tsvd50</th>\n",
       "      <th>new_bow_tsvd51</th>\n",
       "      <th>cos_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>447</td>\n",
       "      <td>20</td>\n",
       "      <td>7654</td>\n",
       "      <td>39672</td>\n",
       "      <td>6105</td>\n",
       "      <td>0.545821</td>\n",
       "      <td>0.106699</td>\n",
       "      <td>-0.005733</td>\n",
       "      <td>-0.132707</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.483006</td>\n",
       "      <td>-9.506209</td>\n",
       "      <td>2.343717</td>\n",
       "      <td>-3.961367</td>\n",
       "      <td>-6.361186</td>\n",
       "      <td>-0.895882</td>\n",
       "      <td>-0.855018</td>\n",
       "      <td>-2.984720</td>\n",
       "      <td>0.415536</td>\n",
       "      <td>0.214325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>216</td>\n",
       "      <td>5</td>\n",
       "      <td>8255</td>\n",
       "      <td>36691</td>\n",
       "      <td>5783</td>\n",
       "      <td>0.321223</td>\n",
       "      <td>-0.081788</td>\n",
       "      <td>-0.059409</td>\n",
       "      <td>-0.033962</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.084099</td>\n",
       "      <td>1.579904</td>\n",
       "      <td>1.914404</td>\n",
       "      <td>-2.640575</td>\n",
       "      <td>1.093888</td>\n",
       "      <td>0.081520</td>\n",
       "      <td>-1.521776</td>\n",
       "      <td>-1.347583</td>\n",
       "      <td>-0.671877</td>\n",
       "      <td>0.171627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>216</td>\n",
       "      <td>5</td>\n",
       "      <td>5191</td>\n",
       "      <td>36691</td>\n",
       "      <td>5783</td>\n",
       "      <td>0.321223</td>\n",
       "      <td>-0.081788</td>\n",
       "      <td>-0.059409</td>\n",
       "      <td>-0.033962</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.084099</td>\n",
       "      <td>1.579904</td>\n",
       "      <td>1.914404</td>\n",
       "      <td>-2.640575</td>\n",
       "      <td>1.093888</td>\n",
       "      <td>0.081520</td>\n",
       "      <td>-1.521776</td>\n",
       "      <td>-1.347583</td>\n",
       "      <td>-0.671877</td>\n",
       "      <td>0.155785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>216</td>\n",
       "      <td>5</td>\n",
       "      <td>4572</td>\n",
       "      <td>36238</td>\n",
       "      <td>5625</td>\n",
       "      <td>0.632411</td>\n",
       "      <td>-0.018920</td>\n",
       "      <td>-0.009728</td>\n",
       "      <td>0.024229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371905</td>\n",
       "      <td>4.228816</td>\n",
       "      <td>18.763676</td>\n",
       "      <td>-20.396324</td>\n",
       "      <td>9.506310</td>\n",
       "      <td>4.371376</td>\n",
       "      <td>-7.526444</td>\n",
       "      <td>-17.522809</td>\n",
       "      <td>9.785788</td>\n",
       "      <td>0.231186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>216</td>\n",
       "      <td>5</td>\n",
       "      <td>3958</td>\n",
       "      <td>41308</td>\n",
       "      <td>6248</td>\n",
       "      <td>0.609788</td>\n",
       "      <td>0.047313</td>\n",
       "      <td>-0.179302</td>\n",
       "      <td>0.069652</td>\n",
       "      <td>...</td>\n",
       "      <td>2.113807</td>\n",
       "      <td>30.442861</td>\n",
       "      <td>11.817908</td>\n",
       "      <td>-22.665281</td>\n",
       "      <td>28.943458</td>\n",
       "      <td>33.333660</td>\n",
       "      <td>-7.465071</td>\n",
       "      <td>-11.551889</td>\n",
       "      <td>-4.041435</td>\n",
       "      <td>0.099988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 267 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gene_len  Gene_lbl_cnc  Var_len  Var_lbl_cnc  text_len  text_word_count  \\\n",
       "0         6           447       20         7654     39672             6105   \n",
       "1         3           216        5         8255     36691             5783   \n",
       "2         3           216        5         5191     36691             5783   \n",
       "3         3           216        5         4572     36238             5625   \n",
       "4         3           216        5         3958     41308             6248   \n",
       "\n",
       "   new_tfidf_tsvd0  new_tfidf_tsvd1  new_tfidf_tsvd2  new_tfidf_tsvd3  \\\n",
       "0         0.545821         0.106699        -0.005733        -0.132707   \n",
       "1         0.321223        -0.081788        -0.059409        -0.033962   \n",
       "2         0.321223        -0.081788        -0.059409        -0.033962   \n",
       "3         0.632411        -0.018920        -0.009728         0.024229   \n",
       "4         0.609788         0.047313        -0.179302         0.069652   \n",
       "\n",
       "     ...     new_bow_tsvd43  new_bow_tsvd44  new_bow_tsvd45  new_bow_tsvd46  \\\n",
       "0    ...          -4.483006       -9.506209        2.343717       -3.961367   \n",
       "1    ...          -1.084099        1.579904        1.914404       -2.640575   \n",
       "2    ...          -1.084099        1.579904        1.914404       -2.640575   \n",
       "3    ...           0.371905        4.228816       18.763676      -20.396324   \n",
       "4    ...           2.113807       30.442861       11.817908      -22.665281   \n",
       "\n",
       "   new_bow_tsvd47  new_bow_tsvd48  new_bow_tsvd49  new_bow_tsvd50  \\\n",
       "0       -6.361186       -0.895882       -0.855018       -2.984720   \n",
       "1        1.093888        0.081520       -1.521776       -1.347583   \n",
       "2        1.093888        0.081520       -1.521776       -1.347583   \n",
       "3        9.506310        4.371376       -7.526444      -17.522809   \n",
       "4       28.943458       33.333660       -7.465071      -11.551889   \n",
       "\n",
       "   new_bow_tsvd51   cos_sim  \n",
       "0        0.415536  0.214325  \n",
       "1       -0.671877  0.171627  \n",
       "2       -0.671877  0.155785  \n",
       "3        9.785788  0.231186  \n",
       "4       -4.041435  0.099988  \n",
       "\n",
       "[5 rows x 267 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TT_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8989"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TT_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = TT_data[0:len(train_var)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = TT_data[len(train_var):8989]\n",
    "y_test = np.zeros((X_test.shape[0],max(y) +1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#y_test = np.zeros((X_test.shape[0],max(y) +1))\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.10968\tvalid-mlogloss:2.12794\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-mlogloss:0.76081\tvalid-mlogloss:1.12096\n",
      "[100]\ttrain-mlogloss:0.448131\tvalid-mlogloss:0.95304\n",
      "[150]\ttrain-mlogloss:0.306317\tvalid-mlogloss:0.915755\n",
      "[200]\ttrain-mlogloss:0.216415\tvalid-mlogloss:0.906394\n",
      "[250]\ttrain-mlogloss:0.158754\tvalid-mlogloss:0.914026\n",
      "[300]\ttrain-mlogloss:0.122072\tvalid-mlogloss:0.928661\n",
      "Stopping. Best iteration:\n",
      "[206]\ttrain-mlogloss:0.20874\tvalid-mlogloss:0.905485\n",
      "\n",
      "0.905485365542\n",
      "[0]\ttrain-mlogloss:2.11092\tvalid-mlogloss:2.12495\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-mlogloss:0.75852\tvalid-mlogloss:1.11784\n",
      "[100]\ttrain-mlogloss:0.437264\tvalid-mlogloss:0.95629\n",
      "[150]\ttrain-mlogloss:0.290224\tvalid-mlogloss:0.922422\n",
      "[200]\ttrain-mlogloss:0.205207\tvalid-mlogloss:0.917367\n",
      "[250]\ttrain-mlogloss:0.149397\tvalid-mlogloss:0.920837\n",
      "Stopping. Best iteration:\n",
      "[195]\ttrain-mlogloss:0.211925\tvalid-mlogloss:0.916505\n",
      "\n",
      "0.916504882088\n",
      "[0]\ttrain-mlogloss:2.10975\tvalid-mlogloss:2.12365\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-mlogloss:0.761862\tvalid-mlogloss:1.05489\n",
      "[100]\ttrain-mlogloss:0.43855\tvalid-mlogloss:0.87512\n",
      "[150]\ttrain-mlogloss:0.30026\tvalid-mlogloss:0.823849\n",
      "[200]\ttrain-mlogloss:0.213793\tvalid-mlogloss:0.807631\n",
      "[250]\ttrain-mlogloss:0.157119\tvalid-mlogloss:0.803999\n",
      "[300]\ttrain-mlogloss:0.116934\tvalid-mlogloss:0.812399\n",
      "Stopping. Best iteration:\n",
      "[238]\ttrain-mlogloss:0.168886\tvalid-mlogloss:0.802449\n",
      "\n",
      "0.802448472978\n",
      "[0]\ttrain-mlogloss:2.10977\tvalid-mlogloss:2.12543\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-mlogloss:0.734775\tvalid-mlogloss:1.12133\n",
      "[100]\ttrain-mlogloss:0.42233\tvalid-mlogloss:0.966262\n",
      "[150]\ttrain-mlogloss:0.283702\tvalid-mlogloss:0.929213\n",
      "[200]\ttrain-mlogloss:0.20024\tvalid-mlogloss:0.924776\n",
      "[250]\ttrain-mlogloss:0.147879\tvalid-mlogloss:0.929959\n",
      "Stopping. Best iteration:\n",
      "[195]\ttrain-mlogloss:0.206932\tvalid-mlogloss:0.924137\n",
      "\n",
      "0.924137474558\n",
      "[0]\ttrain-mlogloss:2.11038\tvalid-mlogloss:2.12129\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-mlogloss:0.757005\tvalid-mlogloss:1.06004\n",
      "[100]\ttrain-mlogloss:0.447542\tvalid-mlogloss:0.888489\n",
      "[150]\ttrain-mlogloss:0.300533\tvalid-mlogloss:0.850464\n",
      "[200]\ttrain-mlogloss:0.210732\tvalid-mlogloss:0.842227\n",
      "[250]\ttrain-mlogloss:0.155424\tvalid-mlogloss:0.842983\n",
      "[300]\ttrain-mlogloss:0.115981\tvalid-mlogloss:0.853442\n",
      "Stopping. Best iteration:\n",
      "[229]\ttrain-mlogloss:0.175966\tvalid-mlogloss:0.841231\n",
      "\n",
      "0.841230895172\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "results=[]\n",
    "denom = 0\n",
    "fold = 5 #Change to 5, 1 for Kaggle Limits\n",
    "for i in range(fold):\n",
    "    params = {\n",
    "        'eta': 0.045,\n",
    "        'max_depth': 5,\n",
    "        'objective': 'multi:softprob',\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'num_class': 9,\n",
    "        'seed': i,\n",
    "        'silent': True\n",
    "    }\n",
    "    x1, x2, y1, y2 = model_selection.train_test_split(X, y, test_size=0.18, random_state=i)\n",
    "    watchlist = [(xgb.DMatrix(x1, y1), 'train'), (xgb.DMatrix(x2, y2), 'valid')]\n",
    "    model = xgb.train(params, xgb.DMatrix(x1, y1), 1000,  watchlist, verbose_eval=50, early_stopping_rounds=100)\n",
    "    score1 = metrics.log_loss(y2, model.predict(xgb.DMatrix(x2), ntree_limit=model.best_ntree_limit), labels = list(range(9)))\n",
    "    print(score1)\n",
    "    \n",
    "    results.append(score1)\n",
    "    if denom != 0:\n",
    "        pred = model.predict(xgb.DMatrix(X_test), ntree_limit=model.best_ntree_limit+80)\n",
    "        preds += pred\n",
    "    else:\n",
    "        pred = model.predict(xgb.DMatrix(X_test), ntree_limit=model.best_ntree_limit+80)\n",
    "        preds = pred.copy()\n",
    "   \n",
    "    denom += 1\n",
    "\n",
    "preds /= denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(preds, columns=['class'+str(c+1) for c in range(9)])\n",
    "submission['ID'] = ID_test\n",
    "submission.head()\n",
    "submission.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
